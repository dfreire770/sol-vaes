{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "#from pathlib import Path\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from sound_dataset import SoundDS\n",
    "\n",
    "import datetime\n",
    "from early_stopping import EarlyStopping\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "from sound_utility import AudioUtil\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn. functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model requires\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.0001\n",
    "max_epoch = 100\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available:\n",
    "    device = torch.device(\"cuda\")    \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "num_workers = 5\n",
    "load_epoch = -1\n",
    "generate = True\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_size=50,num_classes=4):\n",
    "        super(Model,self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_size = latent_size\n",
    "        self.num_classes = num_classes\n",
    "        #(16x161318 and 646694x400)\n",
    "        # For encode\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5)\n",
    "        #self.linear1 = nn.Linear(4*4*32,300)\n",
    "        #self.mu = nn.Linear(300, self.latent_size)\n",
    "        #self.logvar = nn.Linear(300, self.latent_size)\n",
    "        #self.linear1 = nn.Linear(self.input_dim,self.hidden_dim)\n",
    "        self.linear1 = nn.Linear(646694,hidden_dim)\n",
    "        #self.linear1 = nn.Linear(161318,hidden_dim)\n",
    "        \n",
    "        self.mu = nn.Linear(self.hidden_dim, self.latent_size)\n",
    "        \n",
    "        self.logvar = nn.Linear(self.hidden_dim, self.latent_size)\n",
    "        \n",
    "        # For decoder\n",
    "        #self.linear2 = nn.Linear(self.latent_size + self.num_classes, 300)\n",
    "        #self.linear3 = nn.Linear(300,4*4*32)\n",
    "        self.linear2 = nn.Linear(self.latent_size + self.num_classes, self.hidden_dim)\n",
    "        self.linear3 = nn.Linear(self.hidden_dim, self.input_dim)        \n",
    "        self.conv3 = nn.ConvTranspose2d(16, 8, kernel_size=5)\n",
    "        self.conv4 = nn.ConvTranspose2d(8, 1, kernel_size=5, padding=4)\n",
    "        #self.conv5 = nn.ConvTranspose2d(1, 1, kernel_size=5, stride=2, padding=1)\n",
    "\n",
    "\n",
    "    def encode(self,x,y):\n",
    "        y = torch.argmax(y, dim=1).reshape((y.shape[0],1,1))\n",
    "        y = torch.ones(x.shape).to(device)*y\n",
    "        t = torch.cat((x,y),dim=1)        \n",
    "        t = F.relu(self.conv1(t))        \n",
    "        t = F.relu(self.conv2(t))\n",
    "        t = self.Flatten(t)\n",
    "        t = F.relu(self.linear1(t))\n",
    "        mu = self.mu(t)\n",
    "        logvar = self.logvar(t)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "        \n",
    "        #std = torch.exp(logvar) # made a mistake removing 0.5\n",
    "        #epsilon = torch.rand_like(std)\n",
    "        #return mu + std * epsilon\n",
    "            \n",
    "    def Flatten(self,x):\n",
    "        #print('x shape:',x.shape)\n",
    "        batch_size = x.size()[0]\n",
    "        return x.view(batch_size, -1)\n",
    "    \n",
    "    def unFlatten(self, x):\n",
    "        #print('flat x:',x.shape)\n",
    "        batch_size = x.size()[0]\n",
    "        #print('batch_size:',batch_size)\n",
    "        #return x.view(1, batch_size, 4108, 98)\n",
    "        #4097, 87\n",
    "        #return x.reshape((batch_size, 4097, 87))\n",
    "        return x.reshape((batch_size, 4097, 87))\n",
    "        #return x.view(1, batch_size, 4097, 87)\n",
    "\n",
    "    def decode(self, z,y):\n",
    "            \n",
    "        y_reshaped = y.unsqueeze(0).expand(z.shape[0], -1)\n",
    "        inputs = torch.cat([z, y_reshaped], 1)\n",
    "\n",
    "        t = F.relu(self.linear2(inputs))\n",
    "        t = F.relu(self.linear3(t))\n",
    "\n",
    "        t = self.unFlatten(t)\n",
    "        t = F.relu(self.conv3(t))\n",
    "\n",
    "        t = F.relu(self.conv4(t))\n",
    "\n",
    "        return F.tanh(t)\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        mu, logvar = self.encode(x,y)\n",
    "        z = self.reparameterize(mu,logvar)\n",
    "\n",
    "        # Class conditioning\n",
    "        y=y[0]\n",
    "\n",
    "        pred = self.decode(z,y)\n",
    "\n",
    "        return pred, mu, logvar\n",
    "\n",
    "\n",
    "def one_hot(labels, class_size):\n",
    "    targets = torch.zeros(labels.size(0), class_size)\n",
    "    for i, label in enumerate(labels):\n",
    "        targets[i, label] = 1\n",
    "    return targets.to(device)\n",
    "\n",
    "def stft_for_reconstruction(x, fft_size, hopsamp):\n",
    "    window = np.hanning(fft_size)\n",
    "    fft_size = int(fft_size)\n",
    "    hopsamp = int(hopsamp)\n",
    "    return np.array([np.fft.rfft(window*x[i:i+fft_size])\n",
    "                     for i in range(0, len(x)-fft_size, hopsamp)])\n",
    "\n",
    "def istft_for_reconstruction(X, fft_size, hopsamp):\n",
    "    fft_size = int(fft_size)\n",
    "    hopsamp = int(hopsamp)\n",
    "    window = np.hanning(fft_size)\n",
    "    time_slices = X.shape[0]\n",
    "    len_samples = int(time_slices*hopsamp + fft_size)\n",
    "    x = np.zeros(len_samples)\n",
    "    for n,i in enumerate(range(0, len(x)-fft_size, hopsamp)):\n",
    "        x[i:i+fft_size] += window*np.real(np.fft.irfft(X[n]))\n",
    "    return x\n",
    "\n",
    "def reconstruct_signal_griffin_lim(magnitude_spectrogram, fft_size, hopsamp, iterations):\n",
    "    time_slices = magnitude_spectrogram.shape[0]\n",
    "    len_samples = int(time_slices*hopsamp + fft_size)\n",
    "    # Initialize the reconstructed signal to noise.\n",
    "    x_reconstruct = np.random.randn(len_samples)\n",
    "    n = iterations # number of iterations of Griffin-Lim algorithm.\n",
    "    rmse = []\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "        reconstruction_spectrogram = stft_for_reconstruction(x_reconstruct, fft_size, hopsamp)\n",
    "        reconstruction_angle = np.angle(reconstruction_spectrogram)\n",
    "        # Discard magnitude part of the reconstruction and use the supplied magnitude spectrogram instead.\n",
    "        proposal_spectrogram = magnitude_spectrogram*np.exp(1.0j*reconstruction_angle)\n",
    "        prev_x = x_reconstruct\n",
    "        x_reconstruct = istft_for_reconstruction(proposal_spectrogram, fft_size, hopsamp)\n",
    "        diff = np.sqrt(sum((x_reconstruct - prev_x)**2)/x_reconstruct.size)\n",
    "        rmse.append(diff)\n",
    "    return x_reconstruct, rmse\n",
    "\n",
    "\n",
    "def loss_function(x, pred, mu, logvar):\n",
    "    recon_loss = F.mse_loss(pred, x, reduction='sum')\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    #return recon_loss, kld\n",
    "    return recon_loss + kld\n",
    "\n",
    "def train(epoch, model, train_loader, optim):\n",
    "    reconstruction_loss = 0\n",
    "    kld_loss = 0\n",
    "    total_loss = 0\n",
    "    class_size = 4\n",
    "    #for i,(x,y) in enumerate(train_loader):\n",
    "    for batch_idx, (spectrogram,label) in enumerate(train_loader):\n",
    "        try:\n",
    "            #label = np.zeros((x.shape[0], 10))\n",
    "            #label[np.arange(x.shape[0]), y] = 1\n",
    "            #label = torch.tensor(label)\n",
    "\n",
    "            spectrogram_tensor = spectrogram.to(dtype=torch.float32)\n",
    "\n",
    "            label = label.to(device)\n",
    "            data = spectrogram_tensor.to(device)\n",
    "            label = one_hot(label, class_size)\n",
    "\n",
    "            optim.zero_grad()   \n",
    "            pred, mu, logvar = model(data.to(device),label.to(device))\n",
    "            \n",
    "            recon_loss, kld = loss_function(data.to(device),pred, mu, logvar)\n",
    "            loss = recon_loss + kld\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.cpu().data.numpy()*data.shape[0]\n",
    "            reconstruction_loss += recon_loss.cpu().data.numpy()*data.shape[0]\n",
    "            kld_loss += kld.cpu().data.numpy()*data.shape[0]\n",
    "            if i == 0:\n",
    "                print(\"Gradients\")\n",
    "                for name,param in model.named_parameters():\n",
    "                    if \"bias\" in name:\n",
    "                        print(name,param.grad[0],end=\" \")\n",
    "                    else:\n",
    "                        print(name,param.grad[0,0],end=\" \")\n",
    "                    print()\n",
    "        except Exception as e:\n",
    "            #traceback.print_exe()\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    reconstruction_loss /= len(train_loader.dataset)\n",
    "    kld_loss /= len(train_loader.dataset)\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    return total_loss, kld_loss,reconstruction_loss\n",
    "\n",
    "def test(epoch, model, test_loader):\n",
    "    reconstruction_loss = 0\n",
    "    kld_loss = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i,(x,y) in enumerate(test_loader):\n",
    "            try:\n",
    "                label = np.zeros((x.shape[0], 10))\n",
    "                label[np.arange(x.shape[0]), y] = 1\n",
    "                label = torch.tensor(label)\n",
    "\n",
    "                pred, mu, logvar = model(x.to(device),label.to(device))\n",
    "                recon_loss, kld = loss_function(x.to(device),pred, mu, logvar)\n",
    "                loss = recon_loss + kld\n",
    "\n",
    "                total_loss += loss.cpu().data.numpy()*x.shape[0]\n",
    "                reconstruction_loss += recon_loss.cpu().data.numpy()*x.shape[0]\n",
    "                kld_loss += kld.cpu().data.numpy()*x.shape[0]\n",
    "                if i == 0:\n",
    "                    # print(\"gr:\", x[0,0,:5,:5])\n",
    "                    # print(\"pred:\", pred[0,0,:5,:5])\n",
    "                    plot(epoch, pred.cpu().data.numpy(), y.cpu().data.numpy())\n",
    "            except Exception as e:\n",
    "                #traceback.print_exe()\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "    reconstruction_loss /= len(test_loader.dataset)\n",
    "    kld_loss /= len(test_loader.dataset)\n",
    "    total_loss /= len(test_loader.dataset)\n",
    "    return total_loss, kld_loss,reconstruction_loss        \n",
    "\n",
    "def load_data():\n",
    "    \"\"\"transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()])\n",
    "    train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('./data/', train=True, download=True,\n",
    "                             transform=transform),batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('./data/', train=False, download=True,\n",
    "                             transform=transform),batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "    \"\"\"\n",
    "    #dataset = pd.read_csv('./../accordion345.csv')\n",
    "    #dataset = pd.read_csv('./../Violin.csv')\n",
    "    dataset = pd.read_csv('./../Strings.csv')\n",
    "    #dataset = pd.read_csv('./../TinySOL_metadata_updated.csv')\n",
    "\n",
    "    dataset['Full_Path'] = './../TinySOL/' + dataset['Path'].astype(str)\n",
    "    data = dataset[['Full_Path','Instance ID']]\n",
    "    data.head()\n",
    "    \n",
    "    myds = SoundDS(data, './',\n",
    "               duration=1,\n",
    "               sr=44100,\n",
    "               #n_fft=2048,\n",
    "               n_fft=8192,\n",
    "               hop_len=512,\n",
    "               label='Instance ID',\n",
    "               cutoff=128,\n",
    "               highpass=False\n",
    "               )\n",
    "\n",
    "    #batch_size = 32\n",
    "    batch_size = 1\n",
    "\n",
    "\n",
    "    # Random split of 80:20 between training and validation\n",
    "    num_items = len(myds)\n",
    "    num_train = round(num_items * 0.8)\n",
    "    num_val = num_items - num_train\n",
    "    train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "    # Create training and validation data loaders\n",
    "    train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "    val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    return train_dl, val_dl\n",
    "\n",
    "\n",
    "def save_model(model, epoch):\n",
    "    if not os.path.isdir(\"./checkpoints\"):\n",
    "        os.mkdir(\"./checkpoints\")\n",
    "    file_name = './checkpoints/model_{}.pt'.format(epoch)\n",
    "    torch.save(model.state_dict(), file_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using early stopping\n",
    "\n",
    "log_interval = 10\n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "def train_es(epochs, model, train_dl, val_dl, patience = 10):\n",
    "\n",
    "    # Writer will output to ./runs/ directory by default\n",
    "    logdir = './runs/l2_kld_h100_es_ ' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    writer = SummaryWriter(logdir)\n",
    "    \n",
    "    log_interval = 10\n",
    "    lr = 0.0001\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    class_size=4\n",
    "\n",
    "    print(device)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch_idx, (spectrogram,label) in enumerate(train_dl):\n",
    "\n",
    "            #scipy works with cpu when applying the highpass filter\n",
    "            spectrogram_tensor = spectrogram.to(dtype=torch.float32)\n",
    "            #print(\"spectrogram tensor:\",spectrogram_tensor.shape)\n",
    "\n",
    "            label = label.to(device)\n",
    "            data = spectrogram_tensor.to(device)\n",
    "            label = one_hot(label, class_size) # class number, not sure yet \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            recon_batch, mu, logvar = model(data.to(device),label.to(device))\n",
    "            \n",
    "            # recon_batch is prediction, data is ground_truth        \n",
    "            #criterion is for l1 or mse\n",
    "            #loss = criterion(recon_batch, data.view(-1, x_dim))\n",
    "\n",
    "            #print('recon batch:',recon_batch.shape)\n",
    "\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_dl.dataset),\n",
    "                    100. * batch_idx / len(train_dl), loss.item()))\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "            epoch, train_loss / len(train_dl.dataset)))\n",
    "\n",
    "\n",
    "        \"\"\"if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_dl.dataset),\n",
    "                    100. * batch_idx / len(train_dl),\n",
    "                    loss.item() / len(data)))\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "            epoch, train_loss / len(train_dl.dataset)))\"\"\"\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, (spectrogram,labels) in enumerate(val_dl):\n",
    "                float32_tensor = spectrogram.to(dtype=torch.float32)\n",
    "                data, labels = float32_tensor.to(device),labels.to(device)\n",
    "                labels = one_hot(labels, class_size) # \n",
    "\n",
    "                recon_batch, mu, logvar = model(data,labels)\n",
    "                \n",
    "                #val_loss += criterion(recon_batch, data[0]) #.item()\n",
    "                val_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "                #val_loss = criterion(recon_batch, data.view(-1, x_dim))\n",
    "                \n",
    "                #spec = recon_batch.squeeze().cpu().numpy()\n",
    "\n",
    "                #AudioUtil.plot_spectrogram(spec)\n",
    "\n",
    "                #x_rec, rmse = reconstruct_signal_griffin_lim(mag_subset.T, 2048, 512, 30)\n",
    "                #x_rec, rmse = reconstruct_signal_griffin_lim(spec.T, 2048, 512, 30)            \n",
    "\n",
    "                #sf.write(f'./reconstructed_violin_high_pass/test_reconst_{i}.wav', x_rec, 44100)\n",
    "                \n",
    "                #sf.write(f'./reconstructed_accordion/test_reconst_{i}.wav', x_rec, 44100)\n",
    "\n",
    "        #val_loss /= len(val_dl.dataset)\n",
    "\n",
    "        # Calculate average losses\n",
    "        train_loss /= len(train_dl.dataset)\n",
    "        val_loss /= len(val_dl.dataset)\n",
    "        \n",
    "        print('====> Test set loss: {:.4f}'.format(val_loss))\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            print(\"Saving Model\")\n",
    "            save_model(model, epoch)\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_data()\n",
    "print(\"dataloader created\")\n",
    "input_dim=4097*87\n",
    "#input_dim=1025*87\n",
    "hidden_dim=400\n",
    "latent_size=50\n",
    "num_classes=4\n",
    "model = Model(input_dim=input_dim, hidden_dim=hidden_dim, latent_size=latent_size,num_classes=num_classes).to(device)\n",
    "print(\"model created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for batch_idx, (spectrogram,label) in enumerate(train_loader):\n",
    "\n",
    "            #scipy works with cpu when applying the highpass filter\n",
    "    #spectrogram_tensor = spectrogram.to(dtype=torch.float32)\n",
    "    print(spectrogram.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 3\n",
    "#patience = 10\n",
    "#epochs = 100\n",
    "epochs=100\n",
    "train_es(epochs, model, train_dl=train_loader, val_dl=test_loader, patience=patience)\n",
    "#train(epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data_mu = []\n",
    "data_logvar = []\n",
    "targets = []\n",
    "\n",
    "n_samples = int(len(train_loader))\n",
    "counter = 0\n",
    "\n",
    "class_size=4\n",
    "#x_dim = 1025*259\n",
    "x_dim = 4097*87\n",
    "\n",
    "for spectrogram, labels in tqdm(train_loader):\n",
    "#for batch_idx, (spectrogram, labels) in enumerate(train_dl):\n",
    "\n",
    "  float32_tensor = spectrogram.to(dtype=torch.float32)\n",
    "  spec_data, labels_enc = float32_tensor.to(device),labels.to(device)\n",
    "  labels_enc = one_hot(labels_enc, class_size) # \n",
    "\n",
    "  #spec_data = spec_data.view(-1, x_dim)\n",
    "\n",
    "  mu, logvar = model.encode(spec_data,labels_enc)\n",
    "\n",
    "  z = model.reparameterize(mu, logvar).detach().cpu().numpy()\n",
    "  \n",
    "  for x, y in zip(z, labels):\n",
    "    data.append(x)\n",
    "    targets.append(y)\n",
    "    counter += 1\n",
    "\n",
    "  if counter >= n_samples:\n",
    "    break\n",
    "\n",
    "data = np.array(data)\n",
    "targets = np.array(targets)\n",
    "\n",
    "df = pd.DataFrame({\"x\":data[:, 0], \"y\":data[:,1], \"hue\":targets})\n",
    "sns.scatterplot(x=\"x\", y=\"y\", hue=\"hue\", data=df, legend='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(model, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch,val_dl):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (spectrogram, labels) in enumerate(val_dl):\n",
    "            float32_tensor = spectrogram.to(dtype=torch.float32)\n",
    "            data, labels = float32_tensor.to(device),labels.to(device)\n",
    "            labels = one_hot(labels, class_size) # \n",
    "\n",
    "            recon_batch, mu, logvar = model(data, labels)\n",
    "            print('recon_batch:',recon_batch.shape)\n",
    "            #print('label:',labels[0])\n",
    "\n",
    "            #test_loss += criterion(recon_batch, data) #.item()\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "            #if i==0:\n",
    "                #print(spec.shape)\n",
    "                #1025, 431\n",
    "            spec = recon_batch.squeeze().cpu().numpy()\n",
    "            print('spec prev reshape:',spec.shape)\n",
    "            #spec = recon_batch.view(batch_size, 1, 1025, 431).squeeze().cpu().numpy()\n",
    "            \n",
    "            #spec = recon_batch.view(batch_size, 1, 1025, 259).squeeze().cpu().numpy()\n",
    "            \n",
    "            spec = recon_batch.view(batch_size, 1, 4097, 87).squeeze().cpu().numpy()\n",
    "\n",
    "            print('spec reshape:',spec.shape)\n",
    "            print('spec to plot:',spec.shape)\n",
    "            AudioUtil.plot_spectrogram(spec)\n",
    "            print(spec[-1].shape)\n",
    "\n",
    "            #x_rec, rmse = reconstruct_signal_griffin_lim(mag_subset.T, 2048, 512, 30)\n",
    "            4097, 87\n",
    "            #x_rec, rmse = reconstruct_signal_griffin_lim(spec.T, 2048, 512, 30)            \n",
    "\n",
    "            #x_rec, rmse = reconstruct_signal_griffin_lim(spec.T, 8192, 512, 30)            \n",
    "            \n",
    "            x_rec, rmse = reconstruct_signal_griffin_lim(spec.T, 8192, 512, 30)            \n",
    "\n",
    "\n",
    "            #sf.write(f'./reconstructed_accordion/test_reconst_{i}.wav', x_rec, 44100)\n",
    "            sf.write(f'./reconstructed/test_reconst_{i}.wav', x_rec, 44100)\n",
    "\n",
    "    test_loss /= len(val_dl.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "\n",
    "epochs = 15\n",
    "class_size=4\n",
    "for epoch in range(1, epochs + 1):\n",
    "    #c = torch.eye(431,7).cuda()\n",
    "    test(epoch,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new Spectrograms\n",
    "\n",
    "instances = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1],[1,0,1,0]]\n",
    "\n",
    "for instance in instances:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        c = torch.tensor(instance).to(device)\n",
    "\n",
    "        num_samples = 1  # Number of samples to generate\n",
    "        \n",
    "        # Sample from latent space\n",
    "        #sample = torch.randn(num_samples, latent_size).to(device)\n",
    "        z = torch.randn(num_samples, latent_size).to(device)\n",
    "        # Decode samples\n",
    "        recon_batch = model.decode(z, c).cpu() \n",
    "\n",
    "        print(recon_batch.shape)\n",
    "        #torch.Size([1, 1025, 259])\n",
    "    #    spec = recon_batch.view(batch_size, 1, 1025, 431).squeeze().cpu().numpy()\n",
    "\n",
    "        spec = recon_batch.view(batch_size, 1, 4097, 87).squeeze().cpu().numpy()\n",
    "\n",
    "        #spec = recon_batch.view(batch_size, 1, 1025, 259).squeeze().cpu().numpy()\n",
    "        \n",
    "        print(instance)\n",
    "        print(spec.shape)\n",
    "\n",
    "        #noisy\n",
    "        AudioUtil.plot_spectrogram(spec)\n",
    "\n",
    "        #x_rec, rmse = reconstruct_signal_griffin_lim(spec.T, 2048, 512, 30)            \n",
    "        x_rec, rmse = reconstruct_signal_griffin_lim(spec.T, 8192, 512, 30)            \n",
    "        \n",
    "        sf.write(f'./generated/test_gen_{str(instance)}.wav', x_rec, 44100)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Estimate the noise spectrum\n",
    "\n",
    "    \"\"\"\n",
    "        threshold = 0.15\n",
    "        spec[spec < threshold] = threshold\n",
    "\n",
    "        AudioUtil.plot_spectrogram(spec)\n",
    "\n",
    "\n",
    "        x_rec, rmse = reconstruct_signal_griffin_lim(spec.T, 2048, 512, 30)            \n",
    "\n",
    "        sf.write(f'./generated_27/test_gen_fixed_denoised.wav', x_rec, 44100)\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "if load_epoch > 0:\n",
    "    model.load_state_dict(torch.load('./checkpoints/model_{}.pt'.format(load_epoch), map_location=torch.device('mps')))\n",
    "    print(\"model {} loaded\".format(load_epoch))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "for i in range(load_epoch+1, max_epoch):\n",
    "    model.train()\n",
    "    train_total, train_kld, train_loss = train(i, model, train_loader, optimizer)\n",
    "    #with torch.no_grad():\n",
    "    #    model.eval()\n",
    "    #    test_total, test_kld, test_loss = test(i, model, test_loader)\n",
    "    #    if generate:\n",
    "    #        z = torch.randn(6, 32).to(device)\n",
    "    #        y = torch.tensor([1,2,3,4,5,6]) - 1\n",
    "    #        generate_image(i,z, y, model)\n",
    "        \n",
    "    print(\"Epoch: {}/{} Train loss: {}, Train KLD: {}, Train Reconstruction Loss:{}\".format(i, max_epoch,train_total, train_kld, train_loss))\n",
    "    #print(\"Epoch: {}/{} Test loss: {}, Test KLD: {}, Test Reconstruction Loss:{}\".format(i, max_epoch, test_loss, test_kld, test_loss))\n",
    "\n",
    "    save_model(model, i)\n",
    "    train_loss_list.append([train_total, train_kld, train_loss])\n",
    "    #test_loss_list.append([test_total, test_kld, test_loss])\n",
    "    np.save(\"train_loss\", np.array(train_loss_list))\n",
    "    #np.save(\"test_loss\", np.array(test_loss_list))\n",
    "\n",
    "\n",
    "# i, (example_data, exaple_target) = next(enumerate(test_loader))\n",
    "# print(example_data[0,0].shape)\n",
    "# plt.figure(figsize=(5,5), dpi=100)\n",
    "# plt.imsave(\"example.jpg\", example_data[0,0], cmap='gray',  dpi=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiocraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
